{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5wlSUhwotCV"
      },
      "outputs": [],
      "source": [
        "!python -m pip install pika --upgrade\n",
        "!pip install fastapi uvicorn requests python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "owuhvpun2o6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "import json\n",
        "import logging\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "@app.post(\"/webhook\")\n",
        "async def receive_webhook(request: Request):\n",
        "    try:\n",
        "        # Get the JSON payload from the request\n",
        "        payload = await request.json()\n",
        "\n",
        "        # Log the received payload\n",
        "        logging.info(\"Received webhook: %s\", json.dumps(payload, indent=2))\n",
        "\n",
        "        # Extract information from the payload\n",
        "        # The structure will depend on what SharePoint sends, typically:\n",
        "        # {\n",
        "        #     \"value\": [\n",
        "        #         {\n",
        "        #             \"id\": \"unique_id\",\n",
        "        #             \"resource\": \"https://graph.microsoft.com/v1.0/sites/{site-id}/drives/{drive-id}/items/{item-id}\",\n",
        "        #             \"resourceData\": {\n",
        "        #                 \"id\": \"item_id\",\n",
        "        #                 \"name\": \"filename.ext\",\n",
        "        #                 \"webUrl\": \"https://sharepoint_url/file\",\n",
        "        #                 ...\n",
        "        #             }\n",
        "        #         }\n",
        "        #     ]\n",
        "        # }\n",
        "\n",
        "        for notification in payload.get(\"value\", []):\n",
        "            file_info = notification.get(\"resourceData\", {})\n",
        "            item_id = file_info.get(\"id\")\n",
        "            file_name = file_info.get(\"name\")\n",
        "            web_url = file_info.get(\"webUrl\")\n",
        "\n",
        "            # Process the file information as needed\n",
        "            logging.info(f\"File Changed: ID={item_id}, Name={file_name}, URL={web_url}\")\n",
        "\n",
        "        return {\"status\": \"success\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing webhook: {e}\")\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n"
      ],
      "metadata": {
        "id": "lkpkZXbvbrNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=5)\n",
        "def completions_with_backoff(**kwargs):\n",
        "    \"\"\"Handle rate-limited completions from OpenAI.\"\"\"\n",
        "    return openai.Completion.create(**kwargs)\n",
        "\n",
        "\n",
        "# Function to initialize or load vector store\n",
        "def load_or_initialize_vector_store(embeddings, search_query, search_url):\n",
        "    try:\n",
        "        # Attempt to load an existing vector store\n",
        "        vector_store = Chroma(collection_name='chroma_index', persist_directory=persist_directory, embedding_function=embeddings)  # Using Chroma, replace with FAISS if necessary\n",
        "\n",
        "        if vector_store:\n",
        "            return vector_store\n",
        "\n",
        "        else:\n",
        "            print(\"No vector store found, initializing a new one.\")\n",
        "            chunks = load_and_split_document_with_images(search_url)\n",
        "            # Initialize a new vector store\n",
        "            # Save the new vector store\n",
        "            vector_store = generate_embedding(chunks)\n",
        "            return vector_store   # Return the new vector store\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading vector store: {e}\")\n",
        "        # If there's an error, create a new vector store from the provided chunks\n",
        "        chunks = load_and_split_document_with_images(search_url)\n",
        "        # Initialize a new vector store\n",
        "        # Save the new vector store\n",
        "        vector_store = generate_embedding(chunks)\n",
        "        return vector_store  # Return the new vector store\n",
        "\n",
        "def process_pptx_data(pptx_elements):\n",
        "    # Create Document instances\n",
        "\n",
        "    documents = []\n",
        "    for element in pptx_elements:\n",
        "        doc = Document(\n",
        "            page_content=element.page_content,\n",
        "            metadata=element.metadata,\n",
        "            id=str(uuid4())  # Generate a unique ID for each document\n",
        "        )\n",
        "        documents.append(doc)\n",
        "\n",
        "    return documents\n",
        "\n",
        "def process_unstructured_data(yolox_elements, documents):\n",
        "    # Iterate through the elements and extract values based on their type\n",
        "    for element in yolox_elements:\n",
        "        if isinstance(element, Image):\n",
        "          # Debugging statement to show the type of element\n",
        "          print(f\"Processing element of type: {type(element)}\")\n",
        "          print(f\"Image attributes: {dir(element)}\")  # Print all attributes\n",
        "          print(f\"Image data: {element.__dict__}\")  # Inspect instance variables\n",
        "\n",
        "          # Attempt to access and print the file path\n",
        "          if hasattr(element, 'filepath'):\n",
        "              print(f\"Image file path: {element.filepath}\")\n",
        "              image_data = perform_ocr_on_image(element.filepath)\n",
        "              # Add the extracted image data to the document\n",
        "              doc = Document(\n",
        "                  page_content=image_data,\n",
        "                  metadata=element.metadata,\n",
        "                  id=str(uuid4())  # Generate a unique ID for each document\n",
        "              )\n",
        "              documents.append(doc)\n",
        "\n",
        "          # Perform OCR on the image file if the file path exists\n",
        "\n",
        "          elif element.__dict__['text']:\n",
        "              # Add the extracted image data to the document\n",
        "              if hasattr(element.metadata, 'to_dict'):\n",
        "                  metadata_dict = element.metadata.to_dict()  # Convert ElementMetadata to a dictionary\n",
        "              else:\n",
        "                  metadata_dict = element.metadata\n",
        "              doc = Document(\n",
        "                  page_content=element.text,\n",
        "                  metadata=metadata_dict,\n",
        "                  id=str(uuid4())  # Generate a unique ID for each document\n",
        "              )\n",
        "              documents.append(doc)\n",
        "          else:\n",
        "              print(\"No valid information found from the image.\")\n",
        "\n",
        "\n",
        "    # Return the list of extracted values\n",
        "    return documents\n",
        "\n",
        "\n",
        "# Function to load and split the PPTX file\n",
        "def load_and_split_document_with_images(filename):\n",
        "    \"\"\"Load a PPTX document and extract images and text.\"\"\"\n",
        "\n",
        "    # Use UnstructuredPowerPointLoader for structured data\n",
        "    loader = UnstructuredPowerPointLoader(filename, mode=\"elements\")\n",
        "    pptx_elements = loader.load()  # Load the PPTX elements\n",
        "    yolox_elements = partition_pptx_with_yolox(filename)\n",
        "    documents = process_pptx_data(pptx_elements)\n",
        "    documents_with_images = process_unstructured_data(yolox_elements, documents)\n",
        "\n",
        "\n",
        "    return documents_with_images  # Return the documents list\n",
        "\n",
        "\n",
        "# Function to partition PPTX files using Yolox model and extract elements\n",
        "def partition_pptx_with_yolox(filename):\n",
        "    \"\"\"Partition PPTX file using Yolox for high-resolution image processing.\"\"\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        files = shared.Files(\n",
        "            content=f.read(),\n",
        "            file_name=filename,\n",
        "        )\n",
        "\n",
        "    req = shared.PartitionParameters(\n",
        "        files=files,\n",
        "        strategy=shared.Strategy.HI_RES,  # High-resolution strategy\n",
        "        hi_res_model_name=\"yolox\",  # Yolox model\n",
        "        languages =  ['eng', 'ita'], # an error might occur here\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = s.general.partition(req)\n",
        "\n",
        "        img_elements = dict_to_elements(resp.elements)  # Extract elements\n",
        "        return img_elements\n",
        "    except SDKError as e:\n",
        "        print(e)\n",
        "        return []\n",
        "\n",
        "\n",
        "# Function to perform OCR on images\n",
        "def perform_ocr_on_image(image_data):\n",
        "    # If image_data is a file path\n",
        "    if isinstance(image_data, str):\n",
        "        image = PILImage.open(image_data)\n",
        "    # If image_data is binary data, convert it to an image\n",
        "    elif isinstance(image_data, bytes):\n",
        "        image = PILImage.open(BytesIO(image_data))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported image data format.\")\n",
        "\n",
        "    # Perform OCR\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Backoff for embedding generation\n",
        "@backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=10)\n",
        "def generate_embedding(chunks):\n",
        "    \"\"\"Generate embedding for the user query with rate limit handling.\"\"\"\n",
        "    uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
        "\n",
        "\n",
        "    # Save the vector store\n",
        "    vector_store = Chroma(\n",
        "    collection_name=\"chroma_index\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=persist_directory,  # Where to save data locally, remove if not necessary\n",
        "    )\n",
        "    vector_store.add_documents(documents=chunks, ids=uuids)\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "\n",
        "# Define a helper function to handle complex metadata conversion\n",
        "def filter_or_convert_metadata(metadata):\n",
        "    # Iterate through metadata dictionary and process values\n",
        "    for key, value in metadata.items():\n",
        "        if isinstance(value, list):\n",
        "            # Convert lists to comma-separated strings\n",
        "            metadata[key] = ', '.join(map(str, value))\n",
        "        elif isinstance(value, dict):\n",
        "            # Filter out dictionaries or complex types using helper method\n",
        "            metadata[key] = filter_complex_metadata(metadata[key])\n",
        "    return metadata\n",
        "\n",
        "\n",
        "# Function to initialize or load vector store\n",
        "def load_or_initialize_vector_store(embeddings, search_query, search_url):\n",
        "    try:\n",
        "        # Attempt to load an existing vector store\n",
        "        vector_store = Chroma(collection_name='chroma_index', persist_directory=persist_directory, embedding_function=embeddings)  # Using Chroma, replace with FAISS if necessary\n",
        "\n",
        "        if vector_store:\n",
        "            return vector_store\n",
        "\n",
        "        else:\n",
        "            print(\"No vector store found, initializing a new one.\")\n",
        "            chunks = load_and_split_document_with_images(search_url)\n",
        "            # Initialize a new vector store\n",
        "            # Save the new vector store\n",
        "            vector_store = generate_embedding(chunks)\n",
        "            return vector_store   # Return the new vector store\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading vector store: {e}\")\n",
        "        # If there's an error, create a new vector store from the provided chunks\n",
        "        chunks = load_and_split_document_with_images(search_url)\n",
        "        # Initialize a new vector store\n",
        "        # Save the new vector store\n",
        "        vector_store = generate_embedding(chunks)\n",
        "        return vector_store  # Return the new vector store"
      ],
      "metadata": {
        "id": "5Z3eFYxKr3dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Microsoft Graph API credentials\n",
        "client_id = \"YOUR_CLIENT_ID\"\n",
        "client_secret = \"YOUR_CLIENT_SECRET\"\n",
        "tenant_id = \"YOUR_TENANT_ID\"\n",
        "site_id = \"YOUR_SHAREPOINT_SITE_ID\"  # Replace with your actual SharePoint site ID\n",
        "drive_id = \"YOUR_DRIVE_ID\"  # ID of the SharePoint document library\n",
        "\n",
        "# Get OAuth2 token from Azure AD\n",
        "def get_access_token():\n",
        "    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
        "    }\n",
        "    body = {\n",
        "        \"client_id\": client_id,\n",
        "        \"scope\": \"https://graph.microsoft.com/.default\",\n",
        "        \"client_secret\": client_secret,\n",
        "        \"grant_type\": \"client_credentials\"\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=body)\n",
        "    return response.json().get(\"access_token\")\n",
        "\n",
        "# Create a subscription for SharePoint file changes\n",
        "def create_subscription():\n",
        "    token = get_access_token()\n",
        "    url = \"https://graph.microsoft.com/v1.0/subscriptions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Expiration must be within 3 days max\n",
        "    expiration_time = (datetime.utcnow() + timedelta(days=3)).strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "\n",
        "    body = {\n",
        "        \"changeType\": \"updated\",  # can be \"created, deleted\" too\n",
        "        \"notificationUrl\": \"https://your-webhook-url.com/api/notify\",  # replace with your webhook\n",
        "        \"resource\": f\"/sites/{site_id}/drives/{drive_id}/root\",  # replace with your SharePoint resource\n",
        "        \"expirationDateTime\": expiration_time,\n",
        "        \"clientState\": \"yourSecretValue\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=body)\n",
        "    print(\"Subscription Response:\", response.json())\n",
        "\n",
        "# Create the subscription\n",
        "create_subscription()\n"
      ],
      "metadata": {
        "id": "VPtVvDE8HqYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request, HTTPException\n",
        "import requests\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file (if applicable)\n",
        "load_dotenv()\n",
        "\n",
        "# FastAPI instance\n",
        "app = FastAPI()\n",
        "\n",
        "# Microsoft Graph API credentials from environment variables or directly hardcoded\n",
        "client_id = os.getenv(\"CLIENT_ID\", \"YOUR_CLIENT_ID\")\n",
        "client_secret = os.getenv(\"CLIENT_SECRET\", \"YOUR_CLIENT_SECRET\")\n",
        "tenant_id = os.getenv(\"TENANT_ID\", \"YOUR_TENANT_ID\")\n",
        "site_id = os.getenv(\"SITE_ID\", \"YOUR_SHAREPOINT_SITE_ID\")\n",
        "drive_id = os.getenv(\"DRIVE_ID\", \"YOUR_DRIVE_ID\")\n",
        "\n",
        "# Get access token from Azure AD\n",
        "def get_access_token():\n",
        "    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
        "    }\n",
        "    body = {\n",
        "        \"client_id\": client_id,\n",
        "        \"scope\": \"https://graph.microsoft.com/.default\",\n",
        "        \"client_secret\": client_secret,\n",
        "        \"grant_type\": \"client_credentials\"\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, data=body)\n",
        "    if response.status_code != 200:\n",
        "        raise HTTPException(status_code=response.status_code, detail=\"Failed to obtain access token\")\n",
        "    return response.json().get(\"access_token\")\n",
        "\n",
        "# Fetch updated file content from SharePoint\n",
        "def get_file_content(item_id):\n",
        "    token = get_access_token()\n",
        "    url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{drive_id}/items/{item_id}/content\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise HTTPException(status_code=response.status_code, detail=\"Failed to fetch file content\")\n",
        "    return response.content\n",
        "\n",
        "# Mock function for embedding and storing the file\n",
        "def embed_file_and_store(file_content):\n",
        "    # Add your embedding logic here\n",
        "    print(\"Embedding file content:\", file_content)\n",
        "\n",
        "# Notification listener route to receive webhook notifications\n",
        "@app.post(\"/api/notify\")\n",
        "async def notification_listener(request: Request):\n",
        "    data = await request.json()\n",
        "\n",
        "    # Verify that the clientState matches your expected value\n",
        "    if data.get(\"value\")[0].get(\"clientState\") != \"yourSecretValue\":\n",
        "        raise HTTPException(status_code=403, detail=\"Invalid client state\")\n",
        "\n",
        "    # Get the item ID of the changed file\n",
        "    item_id = data.get(\"value\")[0].get(\"resourceData\", {}).get(\"id\")\n",
        "    if not item_id:\n",
        "        raise HTTPException(status_code=400, detail=\"Item ID not found in notification\")\n",
        "\n",
        "    # Fetch the updated file content\n",
        "    file_content = get_file_content(item_id)\n",
        "\n",
        "    # Send the file content to embedding function\n",
        "    embed_file_and_store(file_content)\n",
        "\n",
        "    return {\"status\": \"success\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "S9YBPt4aJpj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import JSONResponse\n",
        "import json\n",
        "import logging\n",
        "import asyncio\n",
        "import queue\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# In-memory queue for batching (you may want to replace this with a persistent queue like RabbitMQ)\n",
        "notification_queue = queue.Queue()\n",
        "BATCH_SIZE = 5  # Example batch size\n",
        "BATCH_INTERVAL = 10  # Seconds\n",
        "\n",
        "async def process_batch():\n",
        "    while True:\n",
        "        if not notification_queue.empty():\n",
        "            batch = []\n",
        "            while not notification_queue.empty() and len(batch) < BATCH_SIZE:\n",
        "                batch.append(notification_queue.get())\n",
        "\n",
        "            # Process the batch\n",
        "            if batch:\n",
        "                await generate_embeddings(batch)\n",
        "\n",
        "        await asyncio.sleep(BATCH_INTERVAL)\n",
        "\n",
        "async def generate_embeddings(batch):\n",
        "    # Here, you'd implement the logic to generate embeddings\n",
        "    # For example, using Hugging Face Transformers\n",
        "    logging.info(\"Generating embeddings for batch: %s\", batch)\n",
        "    # Generate embeddings logic goes here...\n",
        "    # Store embeddings in ChromaDB...\n",
        "\n",
        "@app.post(\"/webhook\")\n",
        "async def receive_webhook(request: Request):\n",
        "    try:\n",
        "        payload = await request.json()\n",
        "        logging.info(\"Received webhook: %s\", json.dumps(payload, indent=2))\n",
        "\n",
        "        # Extract information from payload and put it into the notification queue\n",
        "        for notification in payload.get(\"value\", []):\n",
        "            file_info = notification.get(\"resourceData\", {})\n",
        "            notification_queue.put(file_info)  # Add to the queue\n",
        "\n",
        "        return JSONResponse(status_code=200, content={\"status\": \"success\"})\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing webhook: {e}\")\n",
        "        return JSONResponse(status_code=500, content={\"status\": \"error\", \"message\": str(e)})\n",
        "\n",
        "# Start the batch processing in the background\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    asyncio.create_task(process_batch())\n"
      ],
      "metadata": {
        "id": "QwWRwLdB1mEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Configuration for Microsoft Graph API\n",
        "GRAPH_API_URL = \"https://graph.microsoft.com/v1.0\"\n",
        "ACCESS_TOKEN = \"your_access_token\"  # You would normally retrieve this securely\n",
        "\n",
        "@app.post(\"/create-webhook\")\n",
        "async def create_webhook():\n",
        "    # Define the resource you want to monitor (e.g., a SharePoint document library)\n",
        "    resource = \"https://graph.microsoft.com/v1.0/sites/{site-id}/drives/{drive-id}/root\"\n",
        "    webhook_url = \"https://your_fastapi_endpoint/webhook\"  # Your FastAPI webhook endpoint\n",
        "    expiration = \"2023-12-31T00:00:00.000Z\"  # Set your expiration time (max 4230 minutes)\n",
        "\n",
        "    # Prepare the request payload\n",
        "    subscription_data = {\n",
        "        \"changeType\": \"updated\",\n",
        "        \"notificationUrl\": webhook_url,\n",
        "        \"resource\": resource,\n",
        "        \"expirationDateTime\": expiration,\n",
        "        \"clientState\": \"secretClientValue\"  # Optional client state for validation\n",
        "    }\n",
        "\n",
        "    # Make a POST request to Graph API to create the subscription\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        f\"{GRAPH_API_URL}/subscriptions\",\n",
        "        headers=headers,\n",
        "        json=subscription_data\n",
        "    )\n",
        "\n",
        "    if response.status_code == 201:\n",
        "        logging.info(\"Webhook subscription created successfully.\")\n",
        "        return {\"status\": \"success\", \"data\": response.json()}\n",
        "    else:\n",
        "        logging.error(f\"Failed to create webhook subscription: {response.content}\")\n",
        "        return {\"status\": \"error\", \"message\": response.json()}\n",
        "\n",
        "@app.post(\"/webhook\")\n",
        "async def receive_webhook(request: Request):\n",
        "    try:\n",
        "        payload = await request.json()\n",
        "        logging.info(\"Received webhook: %s\", json.dumps(payload, indent=2))\n",
        "        # Process the payload as needed...\n",
        "        return {\"status\": \"success\"}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing webhook: {e}\")\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n"
      ],
      "metadata": {
        "id": "nxSKWqmw15-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request, HTTPException\n",
        "import pika  # RabbitMQ client library\n",
        "import json\n",
        "import os\n",
        "\n",
        "# FastAPI instance\n",
        "app = FastAPI()\n",
        "\n",
        "# RabbitMQ connection parameters\n",
        "rabbitmq_host = os.getenv(\"RABBITMQ_HOST\", \"your-rabbitmq-server\")\n",
        "rabbitmq_queue = os.getenv(\"RABBITMQ_QUEUE\", \"your_queue_name\")\n",
        "\n",
        "# Endpoint to receive SharePoint webhook notifications\n",
        "@app.post(\"/webhook\")\n",
        "async def handle_webhook(request: Request):\n",
        "    try:\n",
        "        # Get the JSON data from the incoming webhook\n",
        "        data = await request.json()\n",
        "        print(f\"Received webhook data: {data}\")\n",
        "\n",
        "        # Extract relevant data from the webhook payload\n",
        "        subscription_id = data['value'][0].get('subscriptionId')\n",
        "        resource_data = data['value'][0].get('resourceData')\n",
        "        file_id = resource_data.get('Id')\n",
        "        file_name = resource_data.get('Title')\n",
        "        modified_time = resource_data.get('LastModifiedDateTime')\n",
        "\n",
        "        if not file_id or not file_name:\n",
        "            raise HTTPException(status_code=400, detail=\"Invalid data format\")\n",
        "\n",
        "        # Create a message to be sent to RabbitMQ\n",
        "        message = {\n",
        "            'subscriptionId': subscription_id,\n",
        "            'fileId': file_id,\n",
        "            'fileName': file_name,\n",
        "            'modifiedTime': modified_time,\n",
        "            'event': 'file_changed'\n",
        "        }\n",
        "\n",
        "        # Connect to RabbitMQ\n",
        "        connection = pika.BlockingConnection(pika.ConnectionParameters(host=rabbitmq_host))\n",
        "        channel = connection.channel()\n",
        "\n",
        "        # Ensure the queue exists\n",
        "        channel.queue_declare(queue=rabbitmq_queue)\n",
        "\n",
        "        # Publish the message to RabbitMQ\n",
        "        channel.basic_publish(\n",
        "            exchange='',\n",
        "            routing_key=rabbitmq_queue,\n",
        "            body=json.dumps(message)\n",
        "        )\n",
        "\n",
        "        # Close the RabbitMQ connection\n",
        "        connection.close()\n",
        "\n",
        "        # Return a success response\n",
        "        return {\"status\": \"success\", \"message\": \"Webhook processed and message sent to RabbitMQ\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing webhook: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Failed to process webhook\")\n",
        "\n",
        "# If you're running the app using a separate command (like uvicorn), this block is not necessary.\n",
        "# However, to run it as a script for development/testing, you can uncomment the following:\n",
        "# if __name__ == '__main__':\n",
        "#     import uvicorn\n",
        "#     uvicorn.run(app, host=\"0.0.0.0\", port=5000)\n"
      ],
      "metadata": {
        "id": "fjqcsv982Kwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pika\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Initialize the model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def callback(ch, method, properties, body):\n",
        "    # Decode the message\n",
        "    notification = json.loads(body)\n",
        "    # Batch processing logic here\n",
        "    batch.append(notification)\n",
        "\n",
        "    # Process the batch once size threshold is met\n",
        "    if len(batch) >= BATCH_SIZE:\n",
        "        process_batch(batch)\n",
        "        batch.clear()  # Reset batch\n",
        "\n",
        "def process_batch(batch):\n",
        "    texts = [item['name'] for item in batch]\n",
        "    embeddings = model.encode(texts)\n",
        "    # Store embeddings in ChromaDB or another vector database\n",
        "\n",
        "# Set up RabbitMQ connection and queue\n",
        "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
        "channel = connection.channel()\n",
        "channel.queue_declare(queue='file_changes_queue')\n",
        "\n",
        "# Start consuming messages\n",
        "channel.basic_consume(queue='file_changes_queue', on_message_callback=callback, auto_ack=True)\n",
        "print('Waiting for messages...')\n",
        "channel.start_consuming()\n"
      ],
      "metadata": {
        "id": "5nnyVFU81knv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Configuration for Microsoft Graph API\n",
        "GRAPH_API_URL = \"https://graph.microsoft.com/v1.0\"\n",
        "ACCESS_TOKEN = \"your_access_token\"  # You would normally retrieve this securely\n",
        "\n",
        "@app.post(\"/create-webhook\")\n",
        "async def create_webhook():\n",
        "    # Define the resource you want to monitor (e.g., a SharePoint document library)\n",
        "    resource = \"https://graph.microsoft.com/v1.0/sites/{site-id}/drives/{drive-id}/root\"\n",
        "    webhook_url = \"https://your_fastapi_endpoint/webhook\"  # Your FastAPI webhook endpoint\n",
        "    expiration = \"2023-12-31T00:00:00.000Z\"  # Set your expiration time (max 4230 minutes)\n",
        "\n",
        "    # Prepare the request payload\n",
        "    subscription_data = {\n",
        "        \"changeType\": \"updated\",\n",
        "        \"notificationUrl\": webhook_url,\n",
        "        \"resource\": resource,\n",
        "        \"expirationDateTime\": expiration,\n",
        "        \"clientState\": \"secretClientValue\"  # Optional client state for validation\n",
        "    }\n",
        "\n",
        "    # Make a POST request to Graph API to create the subscription\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(\n",
        "        f\"{GRAPH_API_URL}/subscriptions\",\n",
        "        headers=headers,\n",
        "        json=subscription_data\n",
        "    )\n",
        "\n",
        "    if response.status_code == 201:\n",
        "        logging.info(\"Webhook subscription created successfully.\")\n",
        "        return {\"status\": \"success\", \"data\": response.json()}\n",
        "    else:\n",
        "        logging.error(f\"Failed to create webhook subscription: {response.content}\")\n",
        "        return {\"status\": \"error\", \"message\": response.json()}\n",
        "\n",
        "@app.post(\"/webhook\")\n",
        "async def receive_webhook(request: Request):\n",
        "    try:\n",
        "        payload = await request.json()\n",
        "        logging.info(\"Received webhook: %s\", json.dumps(payload, indent=2))\n",
        "        # Process the payload as needed...\n",
        "        return {\"status\": \"success\"}\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing webhook: {e}\")\n",
        "        return {\"status\": \"error\", \"message\": str(e)}\n",
        "\n"
      ],
      "metadata": {
        "id": "iuraJf5Xbv2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to tie everything together\n",
        "def main(file_path, query, max_tokens=100):\n",
        "\n",
        "    # Step 1: Check if vectore exists and if exists then if embedded chunk already exists in the vectorstore. Then Load and split the document, including handling images and OCR\n",
        "    vector_store = load_or_initialize_vector_store(embeddings, query, search_url)\n",
        "    if not vector_store:\n",
        "        print(\"Error: Vector Store not found! Creating and loading...\")\n",
        "        # Update the vector store if no results were found\n",
        "        chunks = load_and_split_document_with_images(file_path)\n",
        "        # Initialize a new vector store\n",
        "        # Save the new vector store\n",
        "        vector_store = generate_embedding(chunks)\n",
        "\n",
        "\n",
        "search_query = \"How many times can you take exams in Italy?\"\n",
        "search_url = \"/main.pptx\"  # Path to your document file\n",
        "main(search_url, search_query)\n"
      ],
      "metadata": {
        "id": "cYffYFEnujrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uvicorn app:app --reload"
      ],
      "metadata": {
        "id": "PteMut8H2uZu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}